{"cells":[{"cell_type":"markdown","id":"3a564d5f","metadata":{"id":"3a564d5f"},"source":["#  Машинный перевод с использованием рекуррентных нейронных сетей\n","\n","__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n","\n","Материалы:\n","* Deep Learning with PyTorch (2020) Авторы: Eli Stevens, Luca Antiga, Thomas Viehmann\n","* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n","* https://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-recurrent-neural-network-pytorch/"]},{"cell_type":"markdown","id":"c9ecd663","metadata":{"id":"c9ecd663"},"source":["## Задачи для совместного разбора"]},{"cell_type":"markdown","id":"05433855","metadata":{"id":"05433855"},"source":["1\\. Рассмотрите пример архитектуры Encoder-Decoder с использованием RNN. Обсудите концепцию teacher forcing."]},{"cell_type":"code","execution_count":1,"id":"HfFAL6gPrLZC","metadata":{"executionInfo":{"elapsed":4419,"status":"ok","timestamp":1700046671176,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"HfFAL6gPrLZC"},"outputs":[],"source":["import torch as th\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":21,"id":"SvmriPBurXrB","metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1700048132522,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"SvmriPBurXrB"},"outputs":[],"source":["n_ru_tokens = 1000\n","batch_size = 16\n","ru_seq_len = 15\n","n_en_tokens = 500\n","en_seq_len = 10\n","\n","ru = th.randint(0, n_ru_tokens, size=(batch_size, ru_seq_len))\n","en = th.randint(0, n_en_tokens, size=(batch_size, en_seq_len))"]},{"cell_type":"code","execution_count":22,"id":"1uwR4gJlrubF","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700048133971,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"1uwR4gJlrubF"},"outputs":[],"source":["class Encoder(nn.Module):\n","  def __init__(self, embedding_dim, hidden_size):\n","      super().__init__()\n","      self.emb = nn.Embedding(\n","          num_embeddings=n_ru_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","      )\n","      self.dropout = nn.Dropout(p=0.5)\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n","\n","  def forward(self, X):\n","    out = self.emb(X) # batch x seq x emb_size\n","    out = self.dropout(out)\n","    _, h = self.rnn(out) # out: batch x seq x hidden_size\n","    return h # 1 x batch x hidden_size"]},{"cell_type":"code","execution_count":23,"id":"A2mQsa1hs-dg","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700048133972,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"A2mQsa1hs-dg"},"outputs":[],"source":["embedding_dim = 100\n","encoder_hidden_size = 300\n","\n","encoder = Encoder(embedding_dim, encoder_hidden_size)\n","\n","encoder_output = encoder(ru)"]},{"cell_type":"code","execution_count":24,"id":"kEnVbLWQtUXe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700048135422,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"kEnVbLWQtUXe","outputId":"8d2e8264-b905-411e-8424-134e41c71643"},"outputs":[{"data":{"text/plain":["torch.Size([1, 16, 300])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["encoder_output.shape"]},{"cell_type":"code","execution_count":27,"id":"ypiaR5DOtYCM","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1700048150110,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"ypiaR5DOtYCM"},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self, embedding_dim, decoder_hidden_size):\n","    super().__init__()\n","    self.emb = nn.Embedding(\n","          num_embeddings=n_en_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","    )\n","    self.rnn = nn.GRUCell(embedding_dim, decoder_hidden_size)\n","    self.fc = nn.Linear(decoder_hidden_size, n_en_tokens)\n","\n","  def forward(self, encoder_output, labels):\n","    # labels: batch x seq_len - считаем, что в 0 столбце SOS\n","    # encoder_output: 1 x batch x encoder_hidden_size\n","    seq_len = labels.size(1)\n","    input_tokens = labels[:, 0]\n","    decoder_hidden = encoder_output[0]\n","    for _ in range(1, seq_len):\n","      out = self.emb(input_tokens).relu() # batch x emb_size\n","      decoder_hidden = self.rnn(out, decoder_hidden) # batch x dec_hidden\n","      out = self.fc(decoder_hidden) # batch x n_en_tokens\n","\n","      # teacher forcing\n","      input_tokens = out.argmax(dim=1).detach()\n","      # ...\n","\n","\n","    # вернуть прогнозы для каждого эл-та последовательности\n","    # batch x seq x n_en_token\n","    return ..."]},{"cell_type":"code","execution_count":29,"id":"eQga1gGsvxbZ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1700048209654,"user":{"displayName":"Никита Блохин","userId":"16402972581398673009"},"user_tz":-180},"id":"eQga1gGsvxbZ","outputId":"cf885238-279a-4111-a9de-a8c9b4d9662b"},"outputs":[{"data":{"text/plain":["Ellipsis"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["decoder = Decoder(embedding_dim=100, decoder_hidden_size=300)\n","decoder(encoder_output, en)"]},{"cell_type":"markdown","id":"4d7b6d63","metadata":{"id":"4d7b6d63"},"source":["## Задачи для самостоятельного решения"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torchmetrics"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torchtext\n","from torch.utils.data import TensorDataset, Dataset, DataLoader\n","import torchtext.transforms as tr\n","import torch.nn as nn"]},{"cell_type":"markdown","id":"20525395","metadata":{"id":"20525395"},"source":["<p class=\"task\" id=\"1\"></p>\n","\n","1\\. Считайте файлы `RuBQ_2.0_train.json` (обучающее множество) и `RuBQ_2.0_test.json` (тестовое множество). Для каждого файла создайте по списка: список предложений на русском языке и список предложений на английском языке. Выведите на экран количество примеров в обучающей и тестовой выборке.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df_train = pd.read_json('data/RuBQ_2.0_train.json')\n","df_test = pd.read_json('data/RuBQ_2.0_test.json')\n","df_test = df_test.loc[:, ['question_text', 'question_eng']]\n","df_train = df_train.loc[:, ['question_text', 'question_eng']]\n","ru_train = df_train['question_text'].values\n","eng_train = df_train['question_eng'].values\n","ru_test = df_test['question_text'].values\n","eng_test = df_test['question_eng'].values"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["((580,), (2330,))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["ru_test.shape, ru_train.shape"]},{"cell_type":"markdown","id":"51d8edd3","metadata":{"id":"51d8edd3"},"source":["<p class=\"task\" id=\"2\"></p>\n","\n","2\\. Создайте два Vocab на основе загруженных данных: `ru_vocab` для слов на русском языке и `en_vocab` для слов на английском языке (словари создаются на основе обучающего множества). Добавьте в словари специальные токены `<PAD>`, `<SOS>`, `<EOS>`. Выведите на экран количество токенов в полученных словарях. Выведите на экран максимальное кол-во слов в предложениях на русском языке и в предложениях на английском языке (в обучающей выборке).\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import re\n","regex = re.compile(r\"[^а-яА-Яa-zA-Z\\s]\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["ru_train = list(map(lambda x: regex.sub('', x).lower().split(), ru_train.tolist()))\n","eng_train = list(map(lambda x: regex.sub('', x).lower().split(), eng_train.tolist()))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["vocab_ru = torchtext.vocab.build_vocab_from_iterator(ru_train, specials=['<unk>', '<pad>', '<sos>', '<eos>'], special_first=[0, 1, 2, 3])\n","vocab_eng = torchtext.vocab.build_vocab_from_iterator(eng_train, specials=['<unk>', '<pad>', '<sos>', '<eos>'], special_first=[0, 1, 2, 3])\n","vocab_ru.set_default_index(0)\n","vocab_eng.set_default_index(0)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["(5825, 4234)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab_ru), len(vocab_eng)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Максимум на русском 25\n","Максимум на eng 31\n"]}],"source":["print(f'Максимум на русском {pd.Series(ru_train).apply(len).max()}')\n","print(f'Максимум на eng {pd.Series(eng_train).apply(len).max()}')\n","max_eng = pd.Series(eng_train).apply(len).max()\n","max_rus = pd.Series(ru_train).apply(len).max()"]},{"cell_type":"markdown","id":"a1d666a1","metadata":{"id":"a1d666a1"},"source":["<p class=\"task\" id=\"3\"></p>\n","\n","3\\. Создайте класс `RuEnDataset`. Реализуйте `__getitem__` таким образом, чтобы он возвращал кортеж `(x, y)`, где x - это набор индексов токенов для предложений на русском языке, а `y` - набор индексов токенов для предложений на английском языке. Используя преобразования, сделайте длины наборов индексов одинаковой фиксированной длины, добавьте в начало каждого набора индекс `<SOS>`, а в конец - индекс токена `<EOS>`. Создайте датасет для обучающей и тестовой выборки.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["X_transform = tr.Sequential(\n","    tr.AddToken(token=\"<sos>\", begin=True),\n","    tr.AddToken(token=\"<eos>\", begin=False),\n","    tr.VocabTransform(vocab_ru),\n","    tr.Truncate(max_seq_len=max_rus),\n","    tr.ToTensor(),\n","    tr.PadTransform(max_length=max_rus, pad_value=1))\n","y_transform = tr.Sequential(\n","    tr.AddToken(token=\"<sos>\", begin=True),\n","    tr.AddToken(token=\"<eos>\", begin=False),\n","    tr.VocabTransform(vocab_eng),\n","    tr.Truncate(max_seq_len=max_eng),\n","    tr.ToTensor(),\n","    tr.PadTransform(max_length=max_eng, pad_value=1)\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class RuEnDataset(Dataset):\n","    def __init__(self, X, y, X_transform, y_transform):\n","        super(RuEnDataset, self).__init__()\n","        self.X_data = X\n","        self.y_data = y\n","        self.X_transform = X_transform\n","        self.y_transform = y_transform\n","    def __getitem__(self, idx):\n","        current_X = self.X_transform(self.X_data[idx])\n","        current_y = self.y_transform(self.y_data[idx])\n","        return current_X, current_y\n","\n","    def __len__(self):\n","        return len(self.X_data)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["ru_test = list(map(lambda x: regex.sub('', x).lower().split(), ru_test.tolist()))\n","eng_test = list(map(lambda x: regex.sub('', x).lower().split(), eng_test.tolist()))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["train_dataset = RuEnDataset(X=ru_train, y=eng_train, X_transform=X_transform, y_transform=y_transform)\n","test_dataset = RuEnDataset(X=ru_test, y=eng_test, X_transform=X_transform, y_transform=y_transform)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([   2,   27,  677, 2206, 5594,    3,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1]),\n"," tensor([   2,    6,  148,  235,   22, 3980,    3,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1]))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[0]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(tensor([   2,   29,    5,  681, 2399,    0, 4758,   90,    0,    0,    3,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1]),\n"," tensor([  2,  10, 224, 142,   7,   0,   0,  16,   0,   0,  15,   3,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1]))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset[1]"]},{"cell_type":"markdown","id":"ca51f9bd","metadata":{"id":"ca51f9bd"},"source":["<p class=\"task\" id=\"4\"></p>\n","\n","4\\. Опишите модель `Encoder`, которая возвращает скрытое состояние рекуррентного слоя в соотстветствии со следующей схемой. Пропустите через эту модель первые 16 предложений на русском языке и выведите размер полученного результата на экран. Результатом должен являться тензор размера `1 x batch_size x hidden_dim` (если используется один однонаправленный рекуррентный слой и `batch_first=True`).\n","\n","* количество эмбеддингов равно количеству слов на русском языке;\n","* размерность эмбеддингов выберите самостоятельно;\n","* при создании слоя эмбеддингов укажите `padding_idx`;\n","* размер скрытого состояния рекуррентного слоя выберите самостоятельно.\n","\n","![encoder](https://i0.wp.com/www.adeveloperdiary.com/wp-content/uploads/2020/10/Machine-Translation-using-Recurrent-Neural-Network-and-PyTorch-adeveloperdiary.com-1.png?w=815&ssl=1)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","  def __init__(self, embedding_dim, hidden_size, n_ru_tokens):\n","      super().__init__()\n","      self.emb = nn.Embedding(\n","          num_embeddings=n_ru_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","      )\n","      self.dropout = nn.Dropout(p=0.5)\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n","\n","  def forward(self, X):\n","    out = self.emb(X) # batch x seq x emb_size\n","    out = self.dropout(out)\n","    _, h = self.rnn(out) # out: batch x seq x hidden_size\n","    return h # 1 x batch x hidden_size"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["encoder = Encoder(embedding_dim=30, n_ru_tokens=len(vocab_ru), hidden_size=20)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 16, 20])\n"]}],"source":["dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n","for X, y in dataloader:\n","    print(encoder(X).shape)\n","    break"]},{"cell_type":"markdown","id":"d94f101d","metadata":{"id":"d94f101d"},"source":["<p class=\"task\" id=\"5\"></p>\n","\n","5\\. Опишите модель `Decoder`, которая возвращает прогноз (набор индексов слов на английском языке). Пропустите через эту модель тензор скрытых состояний кодировщика, полученный в предыдущей задачи, и выведите размер полученного результата на экран. Результатом должен являться тензор размера `batch_size x seq_len x n_en_words` (если используется один однонаправленный рекуррентный слой и `batch_first=True`).\n","\n","* количество эмбеддингов равно количеству слов на английском языке;\n","* размер выходного слоя равен количеству слов на английском языке;\n","* размерность эмбеддингов выберите самостоятельно;\n","* при создании слоя эмбеддингов укажите `padding_idx`;\n","* размер скрытого состояния рекуррентного слоя выберите самостоятельно.\n","\n","![decoder](https://i2.wp.com/www.adeveloperdiary.com/wp-content/uploads/2020/10/Machine-Translation-using-Recurrent-Neural-Network-and-PyTorch-adeveloperdiary.com-2.png?w=899&ssl=1)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self, embedding_dim, decoder_hidden_size, n_en_tokens, epsilon=0.5):\n","    super().__init__()\n","    self.emb = nn.Embedding(\n","          num_embeddings=n_en_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=1\n","    )\n","    self.rnn = nn.GRUCell(embedding_dim, decoder_hidden_size)\n","    self.fc = nn.Linear(decoder_hidden_size, n_en_tokens)\n","    self.epsilon = epsilon\n","\n","\n","  def forward(self, encoder_output, labels):\n","    # labels: batch x seq_len - считаем, что в 0 столбце SOS\n","    # encoder_output: 1 x batch x encoder_hidden_size\n","    seq_len = labels.size(1)\n","    to_ret = []\n","    input_tokens = labels[:, 0]\n","    decoder_hidden = encoder_output[0]\n","    for _ in range(1, seq_len):\n","      out = self.emb(input_tokens).relu() # batch x emb_size\n","      decoder_hidden = self.rnn(out, decoder_hidden) # batch x dec_hidden\n","      out = self.fc(decoder_hidden) # batch x n_en_tokens\n","      to_ret.append(out)\n","      # teacher forcing\n","      if torch.rand(1).item() > self.epsilon:\n","          input_tokens = out.argmax(dim=1).detach()\n","      else:\n","          input_tokens = labels[:, _]\n","\n","\n","    # вернуть прогнозы для каждого эл-та последовательности\n","    # batch x seq x n_en_token\n","    return torch.stack(to_ret).permute(1, 0, 2)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 30, 4234])\n"]}],"source":["decoder = Decoder(embedding_dim=30, decoder_hidden_size=20, n_en_tokens=len(vocab_eng))\n","encoder = Encoder(embedding_dim=30, n_ru_tokens=len(vocab_ru), hidden_size=20)\n","\n","dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n","for X, y in dataloader:\n","    print(decoder(encoder(X), y).shape)\n","    break"]},{"cell_type":"markdown","id":"da2f75c8","metadata":{"id":"da2f75c8"},"source":["<p class=\"task\" id=\"6\"></p>\n","\n","6\\. Объедините модели `Encoder` и `Decoder` в одну модель `EncoderDecoder`. Пропустите через эту модель первые 16 предложений на русском языке и выведите размер полученного результата на экран. Сделайте полученный результат двумерным, объединив две первые размерности: `batch_size * seq_len x n_en_words`. Выведите размерность полученного результата на экран.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder, n_en_tokens):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.n_en_tokens = n_en_tokens\n","\n","\n","    def forward(self, X, labels):\n","        encoder_output = self.encoder(X)\n","        out = self.decoder(encoder_output, labels)\n","        out = out.reshape(-1, self.n_en_tokens)\n","        return out"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([480, 4234])\n"]}],"source":["decoder = Decoder(30, 20, len(vocab_eng))\n","encoder = Encoder(embedding_dim=30, n_ru_tokens=len(vocab_ru), hidden_size=20)\n","encoder_decoder = EncoderDecoder(encoder, decoder, len(vocab_eng))\n","dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n","for X, y in dataloader:\n","    print(encoder_decoder(X, y).shape)\n","    break"]},{"cell_type":"markdown","id":"ec784777","metadata":{"id":"ec784777"},"source":["<p class=\"task\" id=\"7\"></p>\n","\n","7\\. Настройте модель, решив задачу классификации на основе прогнозов модели `EncoderDecoder`. Игнорируйте токен `<PAD>` при расчете ошибки. Во время обучения выводите на экран значения функции потерь для эпохи (на обучающем множестве), значение accuracy по токенам (на обучающем множестве) и пример перевода, сгенерированного моделью. После завершения обучения посчитайте BLEU для тестового множества.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["epochs = 500\n","batch_size = 128\n","device = torch.device('cuda')\n","decoder = Decoder(embedding_dim=150, decoder_hidden_size=100, n_en_tokens=len(vocab_eng))\n","encoder = Encoder(embedding_dim=150, n_ru_tokens=len(vocab_ru), hidden_size=100)\n","model = EncoderDecoder(encoder, decoder, len(vocab_eng)).to(device)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n","loss = nn.CrossEntropyLoss(ignore_index=1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","specials= set(['<unk>', '<pad>', '<sos>', '<eos>'])\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Train loss = 142.4, test loss = 35.68\n","Train Accuracy: 0.046, Test Accuracy: 0.044\n","RU: какие республики входили в ссср\n","EN: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n","__________________________________________________\n","\n","Epoch 26, Train loss = 95.17, test loss = 26.8\n","Train Accuracy: 0.083, Test Accuracy: 0.083\n","RU: кого в англии называют синими воротничками\n","EN: what is the the of\n","__________________________________________________\n","\n","Epoch 51, Train loss = 88.31, test loss = 26.02\n","Train Accuracy: 0.095, Test Accuracy: 0.093\n","RU: как звали коня александра македонского\n","EN: what is the the of the of the\n","__________________________________________________\n","\n","Epoch 76, Train loss = 83.81, test loss = 25.52\n","Train Accuracy: 0.096, Test Accuracy: 0.097\n","RU: в какой стране одно время правили сандинисты\n","EN: what what year is the the the\n","__________________________________________________\n","\n","Epoch 101, Train loss = 77.21, test loss = 24.77\n","Train Accuracy: 0.11, Test Accuracy: 0.12\n","RU: кто сказал и ты брут\n","EN: who is the first of\n","__________________________________________________\n","\n","Epoch 126, Train loss = 70.95, test loss = 24.67\n","Train Accuracy: 0.12, Test Accuracy: 0.12\n","RU: в каком направлении пел синатра\n","EN: in what direction did the\n","__________________________________________________\n","\n","Epoch 151, Train loss = 66.97, test loss = 24.32\n","Train Accuracy: 0.13, Test Accuracy: 0.13\n","RU: какой британский нововолновый дуэт исполнял популярный хит video killed the radio star видео погубило звезду радио\n","EN: what is the name of the the of the of the the of of\n","__________________________________________________\n","\n","Epoch 176, Train loss = 62.03, test loss = 24.47\n","Train Accuracy: 0.14, Test Accuracy: 0.13\n","RU: что карл украл у клары\n","EN: what is the graduate play\n","__________________________________________________\n","\n","Epoch 201, Train loss = 58.08, test loss = 24.66\n","Train Accuracy: 0.14, Test Accuracy: 0.13\n","RU: когда умер стив джобс\n","EN: when was the invented die\n","__________________________________________________\n","\n","Epoch 226, Train loss = 56.85, test loss = 24.81\n","Train Accuracy: 0.15, Test Accuracy: 0.13\n","RU: кто организовал компанию mercedes\n","EN: who is the company\n","__________________________________________________\n","\n","Epoch 251, Train loss = 51.41, test loss = 24.86\n","Train Accuracy: 0.16, Test Accuracy: 0.13\n","RU: при какой температуре испаряется бензин\n","EN: how what temperature does water evaporate\n","__________________________________________________\n","\n","Epoch 276, Train loss = 48.73, test loss = 25.56\n","Train Accuracy: 0.16, Test Accuracy: 0.13\n","RU: что коллекционирует голливудский актр харрисон форд\n","EN: what did the name harrison consist collect\n","__________________________________________________\n","\n","Epoch 301, Train loss = 45.94, test loss = 25.17\n","Train Accuracy: 0.17, Test Accuracy: 0.14\n","RU: какая станция московского метрополитена до года носила название лермонтовская\n","EN: what is the name of the highest point in of the\n","__________________________________________________\n","\n","Epoch 326, Train loss = 43.98, test loss = 25.98\n","Train Accuracy: 0.18, Test Accuracy: 0.14\n","RU: в каком году открыта америка колумбом\n","EN: in what year was the is the\n","__________________________________________________\n","\n","Epoch 351, Train loss = 40.59, test loss = 25.6\n","Train Accuracy: 0.19, Test Accuracy: 0.14\n","RU: за какой клуб выступал бразильский футболист гарринча\n","EN: which country did the famous footballer garrincha play for\n","__________________________________________________\n","\n","Epoch 376, Train loss = 38.02, test loss = 25.86\n","Train Accuracy: 0.19, Test Accuracy: 0.14\n","RU: в какой латиноамериканской стране в году победила сандинистская революция\n","EN: in which country did the famous storyteller builder rule in\n","__________________________________________________\n","\n","Epoch 401, Train loss = 35.91, test loss = 26.1\n","Train Accuracy: 0.2, Test Accuracy: 0.14\n","RU: в каком океане находится марианская впадина\n","EN: in what country is the birthplace of located\n","__________________________________________________\n","\n","Epoch 426, Train loss = 34.92, test loss = 26.66\n","Train Accuracy: 0.21, Test Accuracy: 0.14\n","RU: чьей дочерью является дейнерис\n","EN: whose son is daenerys\n","__________________________________________________\n","\n","Epoch 451, Train loss = 31.75, test loss = 26.82\n","Train Accuracy: 0.22, Test Accuracy: 0.14\n","RU: какой город является столицей новой зеландии\n","EN: which city is the capital of australia zealand located\n","__________________________________________________\n","\n","Epoch 476, Train loss = 30.61, test loss = 27.09\n","Train Accuracy: 0.22, Test Accuracy: 0.14\n","RU: на какой реке находится остров тагарский\n","EN: on which river is the city of naberezhnye tagar\n","__________________________________________________\n","\n"]}],"source":["for i in range(epochs):\n","    train_accuracy = torchmetrics.Accuracy(num_classes=len(vocab_eng), task='multiclass').to(device)\n","    test_accuracy = torchmetrics.Accuracy(num_classes=len(vocab_ru), task='multiclass').to(device)\n","\n","    model.train()\n","    train_loss = 0\n","    test_loss = 0\n","    for current_X, current_y in train_dataloader:\n","        current_X = current_X.to(device)\n","        current_y = current_y.to(device)\n","        pred = model(current_X, current_y)\n","        current_y = current_y[:, 1:] # так как предикты без <sos>\n","        error = loss(pred, current_y.flatten().long())\n","        arg_pred = torch.argmax(pred, dim = 1)\n","        train_accuracy.update(arg_pred, current_y.flatten().long())\n","        error.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        train_loss += error.item()\n","\n","    model.eval()\n","    for current_X, current_y in test_dataloader:\n","        current_X = current_X.to(device)\n","        current_y = current_y.to(device)\n","        pred = model(current_X, current_y)\n","        current_y = current_y[:, 1:]\n","        error = loss(pred, current_y.flatten().long())\n","        arg_pred = torch.argmax(pred, dim = 1)\n","        test_accuracy.update(arg_pred, current_y.flatten().long())\n","        test_loss += error.item()\n","    if not i%25:\n","        model.eval()\n","        index = torch.randint(0, len(train_dataset), size=(1,))\n","        eval_X, eval_y = train_dataset[index.item()]\n","        eval_X, eval_y = eval_X.to(device).unsqueeze(0), eval_y.to(device).unsqueeze(0)\n","        pred = model(eval_X, eval_y)\n","        indexes = torch.argmax(pred, dim=1)\n","\n","        ru_tokens = list(map(lambda x: vocab_ru.get_itos()[x], eval_X[0].cpu().detach().numpy().tolist()))\n","        en_tokens = list(map(lambda x: vocab_eng.get_itos()[x], indexes.cpu().detach().numpy().tolist()))\n","        ru_tokens = ' '.join(list(filter(lambda x: x not in specials, ru_tokens\n","                                         )))\n","        en_tokens = ' '.join(list(filter(lambda x: x not in specials,en_tokens)))\n","\n","        print(f'Epoch {i+1}, Train loss = {train_loss:.4}, test loss = {test_loss:.4}')\n","        print(f'Train Accuracy: {train_accuracy.compute():.2}, Test Accuracy: {test_accuracy.compute():.2}')\n","\n","        print(f'RU: {ru_tokens}')\n","        print(f'EN: {en_tokens}')\n","        print('_'*50 + '\\n')\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["res = []\n","true = []\n","model.eval()\n","for current_X, current_y in test_dataloader:\n","        current_X = current_X.to(device)\n","        current_y = current_y.to(device)\n","        pred = model(current_X, current_y)\n","        current_y = current_y[:, 1:]\n","        indexes = torch.argmax(pred, dim=1)\n","        current_y = current_y.flatten().cpu().detach().numpy().tolist()\n","        en_tokens = list(map(lambda x: vocab_eng.get_itos()[x], indexes.cpu().detach().numpy().tolist()))\n","        en_tokens = list(filter(lambda x: x not in set(['<eos>', '<sos>', '<pad>']), en_tokens))\n","        res.append(en_tokens)\n","        true_tokens = list(map(lambda x: vocab_eng.get_itos()[x], current_y))\n","        true_tokens = list(filter(lambda x: x not in set(['<eos>', '<sos>', '<pad>']), true_tokens))\n","        true.append([true_tokens])\n"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["0.2000749111175537"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["from torchtext.data.metrics import bleu_score\n","bleu_score(res, true)"]},{"cell_type":"markdown","id":"66caa919","metadata":{"id":"66caa919"},"source":["## Обратная связь\n","- [ ] Хочу получить обратную связь по решению"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
